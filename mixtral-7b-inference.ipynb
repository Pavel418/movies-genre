{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n","ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n","ggml_init_cublas: found 2 CUDA devices:\n","  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n","  Device 1: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n","llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/p.kuznetsov/script/test/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 17\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n","llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q5_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 32768\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 32768\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n","llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.33 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =    85.94 MiB\n","llm_load_tensors:      CUDA0 buffer size =  2495.28 MiB\n","llm_load_tensors:      CUDA1 buffer size =  2311.77 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: freq_base  = 1000000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =   136.00 MiB\n","llama_kv_cache_init:      CUDA1 KV buffer size =   120.00 MiB\n","llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n","llama_new_context_with_model: graph splits (measure): 5\n","llama_new_context_with_model:      CUDA0 compute buffer size =   160.00 MiB\n","llama_new_context_with_model:      CUDA1 compute buffer size =   160.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    12.00 MiB\n"]}],"source":["from llama_cpp import Llama\n","fast_llm = Llama(\n","  model_path=\"/home/p.kuznetsov/script/test/mistral-7b-instruct-v0.2.Q5_K_M.gguf\", \n","  n_ctx=2048,  # lets not be greedy\n","  n_threads=4,           \n","  n_gpu_layers=-1,\n","  main_gpu = 1,\n","  verbose=False\n",")"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/p.kuznetsov/script/test/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 17\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n","llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q5_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 32768\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 32768\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n","llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.33 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =    85.94 MiB\n","llm_load_tensors:      CUDA0 buffer size =  2495.28 MiB\n","llm_load_tensors:      CUDA1 buffer size =  2311.77 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 5600\n","llama_new_context_with_model: freq_base  = 1000000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =   371.88 MiB\n","llama_kv_cache_init:      CUDA1 KV buffer size =   328.12 MiB\n","llama_new_context_with_model: KV self size  =  700.00 MiB, K (f16):  350.00 MiB, V (f16):  350.00 MiB\n","llama_new_context_with_model: graph splits (measure): 5\n","llama_new_context_with_model:      CUDA0 compute buffer size =   388.94 MiB\n","llama_new_context_with_model:      CUDA1 compute buffer size =   388.94 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    18.94 MiB\n"]}],"source":["from llama_cpp import Llama\n","larger_llm = Llama(\n","  model_path=\"/home/p.kuznetsov/script/test/mistral-7b-instruct-v0.2.Q5_K_M.gguf\", \n","  n_ctx=5600,  # lets not be greedy\n","  n_threads=4,           \n","  n_gpu_layers=-1,\n","  main_gpu = 1,\n","  verbose=False\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","import json\n","\n","def genre(text, genres):\n","    genres_text = \", \".join(str(item) for item in genres)\n","    \n","    prompt = f\"\"\"Given a movie plot summary identify the single most predominant genre from the following list: {genres_text}.\n","    Your output should be a formatted json with the field \"genre\" set to the identified genre from the list. Include only one genre.\n","Example 1:\n","Input: In a dystopian future, a young hacker stumbles upon a government conspiracy and must risk everything to expose the truth. \n","Output: {{\"genre\":\"Triller\"}}\n","Example 2:\n","Input: A clumsy, aspiring chef inherits a struggling restaurant and, with the help of her quirky family, whips up delicious dishes and heartwarming community spirit.\n","Output: {{\"genre\":\"Comedy\"}}\n","Example 3:\n","Input: A seasoned detective investigates a series of gruesome murders and races against time to stop the killer before they strike again.\n","Output: {{\"genre\":\"Crime Thriller\"}}\n","\n","You will output in JSON format, without any other text.\n","\"\"\"\n","    \n","    import tiktoken\n","\n","    encoding = tiktoken.get_encoding(\"cl100k_base\")\n","    def count_tokens(text):\n","        return len(encoding.encode(text))\n","\n","    # Count the tokens in the system message and user content\n","    system_tokens = count_tokens(prompt)\n","    user_tokens = count_tokens(\"Sentence:\"+text)\n","\n","    # Calculate the total prompt tokens\n","    total_tokens = system_tokens + user_tokens\n","    \n","    if total_tokens > 1700:\n","        llm = larger_llm\n","    else:\n","        llm = fast_llm\n","    messages = [\n","        {\"role\": \"system\", \"content\": prompt},\n","        {\"role\": \"user\", \"content\": \"Sentence:\"+text}\n","    ]\n","    iterations = 0\n","    while iterations < 8:\n","        iterations += 1\n","        resp = llm.create_chat_completion(\n","            messages=messages,\n","            temperature = 0.01,\n","            max_tokens = 100,\n","        )\n","        response = resp['choices'][0]['message']['content']\n","        try:\n","            match = re.search(r'{(.+?)}', response)\n","            if match:\n","                response = \"{\" + match.group(1) + \"}\"\n","            else:\n","                raise json.JSONDecodeError(\"No json found\", response, 0)\n","\n","            genre = json.loads(response)['genre']\n","            if genre in genres:\n","                return genre.replace(\",\", \"\")\n","            else:\n","                print(\"Genre not in list\")\n","                print(response)\n","                messages.append(\n","                    {\n","                        \"role\": \"user\",\n","                        \"content\": f\"Please select only one genre from the list with the exact wording: {genres_text}\"\n","                    }\n","                )\n","        except json.JSONDecodeError:\n","            print(\"JSON Decode Error\")\n","            print(response)\n","            messages.append(\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": \"Please follow the json format, do not include any other text.\"\n","                }\n","            )\n","    return resp[\"choices\"][0][\"message\"][\"content\"]"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","train = pd.read_csv('/home/p.kuznetsov/script/test/train_data.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["column_names = train.columns\n","\n","# Columns to remove\n","columns_to_remove = ['id', 'plot', 'title']\n","\n","# Create a new list of column names without the specified columns\n","genres = [col for col in column_names if col not in columns_to_remove]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.read_csv('test_features.csv')\n","test = test.head(100)\n","for column in genres:\n","    test[column] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-23T07:59:44.675263Z","iopub.status.idle":"2024-01-23T07:59:44.675619Z","shell.execute_reply":"2024-01-23T07:59:44.675451Z","shell.execute_reply.started":"2024-01-23T07:59:44.675434Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","start = time.time()\n","test['result'] = test['plot'].apply(lambda x: genre(x, genres))\n","end = time.time()\n","\n","errors = 0\n","def update_column(row):\n","    column_to_update = row['result']\n","    if column_to_update in genres:\n","        test.at[row.name, column_to_update] = 1\n","    else:\n","        global errors\n","        errors += 1\n","\n","# Apply the function to each row\n","test.apply(lambda row: update_column(row), axis=1)\n","\n","# Drop the temporary 'result' column\n","# test = test.drop('result', axis=1)\n","\n","print(errors)\n","print(end - start)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Genre not in list\n","{\"genre\":\"Thriller, Crime Fiction\"}\n","Genre not in list\n","{\"genre\":\"Mystery, Crime Thriller\"}\n","JSON Decode Error\n"," Based on the given input, the genre would be: \"Science Fiction\".\n","0.2608695652173913\n"]}],"source":["y_pred = pd.DataFrame(columns=genres)\n","train = train.head(10)\n","for index, row in train.iterrows():\n","    predicted = genre(row['plot'], genres)\n","    y_pred.at[index, predicted] = 1\n","\n","from sklearn.metrics import f1_score\n","y_pred = y_pred.fillna(0)\n","score = f1_score(train[genres], y_pred[genres], average='micro')\n","print(score)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4341541,"sourceId":7458684,"sourceType":"datasetVersion"},{"datasetId":4341592,"sourceId":7458810,"sourceType":"datasetVersion"},{"sourceId":159449453,"sourceType":"kernelVersion"},{"sourceId":159451933,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"myenv","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
